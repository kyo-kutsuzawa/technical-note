---
title: "Real numbers, data science and chaos: How to fit any dataset with a single parameter"
---

Laurent Boué, "Real numbers, data science and chaos: How to fit any dataset with a single parameter," arXiv preprint, 2019. [Online Available](https://arxiv.org/abs/1904.12320)

# 要約

どんなモダリティのどんなデータも，以下の関数のパラメータ$\alpha$を変えるだけで近似できる。
$$
f(\alpha) = \sin^2 (2^{x \tau} \arcsin \sqrt{\alpha})
$$
ここで $x$ は離散時刻，$\tau$ は精度パラメータ。

# 原理

論文では音声と画像を例に挙げて，さまざまなデータが1つのパラメータ $\alpha$ だけで近似できる様子を示している。
以下に大まかな原理を示す。

近似の前に，まずはデータを有限個の実数の列 $[x_1, x_2, \ldots, x_n]$ として表現してやる。
これは基本的にどんなデータでも可能である。
音声データは離散時間上での振幅値の列とみなせるし，画像データも各ピクセルの色の値を左上のピクセルから順に並べていけば実数列として表現できる。
さらにこの実数列を各値を最小値と最大値で正規化して， $x_k$ が単位区間上（0以上1未満）に値をとるようにしてやる。

こうして得られた実数列の各要素を $\tau$ ビットの固定小数点数で表現して，それらをひとつなぎに並べてしまえば，最終的にデータを $n\tau$ ビットの固定小数点数として表現できてしまう。
イメージとしては，$[0.1234, 0.5678, \ldots, 0.9012]$ という実数列があったとき，それらの小数部分を並べてひとつの数値 $0.12345678\ldots9012$ をつくる感じである。
これが，任意のデータを1つのパラメータで近似できる原理である。

パラメータ $\alpha_0$ からは元のデータを復元することもできる。
データの $k$ 番目の要素 $x_k$ を復元するには以下の式を使えばよい：
$$
x_k \approx \alpha_k = 2^{k\tau}\alpha_0 \mod 1
$$
この処理は，パラメータを $k\tau$ ビットだけシフトさせたあと1以上の数値を除くことをしている。
この処理での近似誤差は，実数列を有限桁数にしたときの量子化誤差 $2^{-\tau}$ 未満である。

以上のようにしてデータからパラメータへの変換（エンコード）とパラメータからデータの復元（デコード）が実現できる。
論文には疑似コードでの実装例も紹介している。

# 連続かつ微分可能な関数での表現

上記の方法は今風（深層学習の時代風？）じゃない。
ということで，デコード部分を連続かつ微分可能な処理でできるように書き直す。
邪魔なのは整数部分を切り捨てる $\mathrm{mod}$ の計算なので，これをどうにか連続な関数に置き換えたい。

そのために鍵になるのは以下の関数 $\phi$
$$
\phi(\alpha) = \sin^2 (2 \pi \alpha)
$$
とその逆関数 $\phi^{-1}$
$$
\phi^{-1}(z) = \frac{1}{2\pi} \arcsin \sqrt{z}
$$
である。
これらの関数はいずれも連続かつ微分可能である。

具体的な処理を見ていく。
まずはエンコード部分，すなわち実数列 $[x_0, \ldots, x_n]$ をパラメータ $z_0$ に変換するところから。
まずは実数列の各要素に $\phi^{-1}$ を適用してやる。
そのあと，$[\phi^{-1}(x_0), \ldots, \phi^{-1}(x_n)]$ の各要素 $\phi^{-1}(x_k)$ を $\tau$ ビットで近似してやる。
それらをひと続きに並べて $\alpha_0$ としたら，さらに $\phi$ を適用して $z_0 = \phi(\alpha_0)$ を得る。
こうして得られた $z_0$ がパラメータである。

続いてデコード部分。
以下の処理をすればパラメータ $z_0$ からデータの要素 $x_k$ を復元できる：
$$
x_k \approx z_k = \sin^2 (2^{k \tau} \arcsin \sqrt{z_0})
$$
このとき近似誤差は $\frac{\pi}{2^{\tau - 1}}$ 未満である。
さらに，この処理は連続かつ微分可能である。
完！

## 汎化能力

するわけがない。
今回の手法で得られるパラメータは単にデータを丸暗記しているだけで，汎化のようなものは期待できない。
たとえば時系列に適用したとしても将来の予測には使えない。

一方で深層学習でも，ニューラルネットワーク（NN）はランダムなデータを学習できる（つまり丸暗記できる）ことが知られている。
では，NNはどのくらいデータを丸暗記していて，どのくらい意味ある学習をしているのか？
どうやって汎化的なことをしているのか？

# 感想とか

- おもしろいネタ論文。パラメータの数が少なきゃいいってものではない。
- やっぱ学習する上で重要なのは汎化だよねという気持ちになる。
